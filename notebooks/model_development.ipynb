{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load processed dataset and prepare our training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_path = \"../data/processed/training_boston.csv\"\n",
    "test_set_path = \"../data/processed/test_boston.csv\"\n",
    "\n",
    "training_set = pd.read_csv(training_set_path)\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "\n",
    "X_train = training_set.drop(columns = [\"MEDV\"])\n",
    "X_test = test_set.drop(columns = [\"MEDV\"])\n",
    "y_train = training_set[\"MEDV\"]\n",
    "y_test = test_set[\"MEDV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our gradient boosting model (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(random_state = 492)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance metrics of our value\n",
    "* Loss value and R2 (correlation coefficient) value is computed based on the default values of hyperparameters. We'll use an hyperparameter optimization technique such as Grid Search to improve the accuracy and generalizability of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RMSE: 3.593491843441745\n",
      "Model R²: 0.8419638129500432\n"
     ]
    }
   ],
   "source": [
    "# rmse value as loss\n",
    "loss = root_mean_squared_error(y_test, y_pred)\n",
    "r2_val = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model RMSE: {loss}\")\n",
    "print(f\"Model R²: {r2_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization\n",
    "* Implemented a grid search and obtained better (as the result of performance metrics suggest) hyperparameters for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 500, 'subsample': 0.6}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_estimators - # of trees (m in the equation)\n",
    "learning_rate - gamma in the equation (step size shrinkage)\n",
    "max_depth - max depth of each tree that will be created during boosting process\n",
    "min_child_weight - prevent overfitting (define a threshold for node splitting based on sum of child weights)\n",
    "subsample - ratio of data points going to be used while creating each tree (1.0 -> whole training data)\n",
    "colsample_bytree - fraction of features going to be used while creating each tree (introduce randomness and prevent ofitting)\n",
    "\"\"\"\n",
    "\n",
    "# param_grid -> defined dictionary that is going to be used for grid search (hyperparam optim)\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300, 500],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# n_jobs param -> # of processors going to be used (-1 : all)\n",
    "# cv -> cross validation strategy (5 : default)\n",
    "# scoring -> strategy to evaluate performance of CV (takes a scoring parameter)\n",
    "grid_search = GridSearchCV(estimator=XGBRegressor(random_state=42), \n",
    "                           param_grid=param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance metrics of the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model RMSE: 3.2981723706179418\n",
      "Best model R²: 0.8668718446328416\n"
     ]
    }
   ],
   "source": [
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "best_model_loss = root_mean_squared_error(y_test, y_pred_best)\n",
    "best_model_r2 = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best model RMSE: {best_model_loss}\")\n",
    "print(f\"Best model R²: {best_model_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (House Price Pred)",
   "language": "python",
   "name": "house_price_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
